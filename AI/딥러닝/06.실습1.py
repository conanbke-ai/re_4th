# 로짓 (Logits) : Softmax를 거치기 전의 원시 출력값
# 예 : [2.1, 0.5, -0.3]-> 아직 확률이 아님

# Normalize: 데이터를 평균 0, 표준편차 1로 변환
# (0.1307, 0.3081) : MNIST 데이터셋의 평균과 표준 편차
# - 학습 안정성 향상

# Dropout: 학습 시 일부 뉴련을 무작위로 끄는 기법
# - nn.Dropout(0.2): 20%의 뉴련을 랜덤하게 비활성화
# - 과적합 방지 효과

# model.train() vs model.eval()
# - train(): 학습 모드 (Dropout 작동)
# - eval(): 평가 모드 (Dropout 비활성화)

# torch.no_grad() : 기울기 계산 비활성화
# - 추론/ 평가 시 사용
# - 메모리 절약 + 속도 향상

# DataLoader: 데이터를 배치로 나눠서 제공
# batch_size=64: 한번에 64개씩 처리
# shuffle=True: 매 애폭마다 데이터 순서 섞기

# 애폭 (Epoch): 전체 데이터를 1회 학습
# - 10 epochs = 60,000개 데이터를 10번 학습

# 배치 (Batch): 한 번에 처리하는 데이터 묶음
# - 전체를 한번에 처리하면 메모리 부족
# - 작은 배치로 나눠서 처리